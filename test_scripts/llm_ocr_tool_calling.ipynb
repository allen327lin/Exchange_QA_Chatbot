{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR Tool Calling\n",
    "Agent: https://python.langchain.com/docs/tutorials/agents/\n",
    "\n",
    "Tool calling: https://python.langchain.com/docs/concepts/tool_calling/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 刪除環境變量\n",
    "if \"GROQ_API_KEY\" in os.environ:\n",
    "    del os.environ[\"GROQ_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/allen/Documents/code/Exchange_QA_Chatbot\n",
      "Successfully loaded env variables: True\n"
     ]
    }
   ],
   "source": [
    "# 找根目錄\n",
    "def find_project_root(current_path, marker=\".git\"):\n",
    "    current_path = Path(current_path).resolve()\n",
    "    for parent in current_path.parents:\n",
    "        if (parent / marker).exists():\n",
    "            return parent\n",
    "    return None\n",
    "\n",
    "current_path = os.getcwd()\n",
    "project_root = find_project_root(current_path, marker=\".git\")\n",
    "print(\"Project root:\", project_root)\n",
    "\n",
    "# Load .env file\n",
    "print(f\"Successfully loaded env variables: {load_dotenv(project_root / \".env\")}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded env variables:\n",
      "GROQ_API_KEY = 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\n"
     ]
    }
   ],
   "source": [
    "# Load env variables into python variables\n",
    "print(\"Loaded env variables:\")\n",
    "print(f\"GROQ_API_KEY = {os.getenv(\"GROQ_API_KEY\")}\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    "    temperature=0.7,\n",
    "    max_retries=2,\n",
    "    api_key=GROQ_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def image_ocr(image_file_path: str) -> str:\n",
    "    \"\"\"Perform Optical Character Recognition (OCR) on an image to extract text.\n",
    "\n",
    "    Args:\n",
    "        image_file_path: the image file path\n",
    "    \"\"\"\n",
    "    # do easyocr\n",
    "    image_text = [image_file_path, [\"Title\", \"Content\", \"Page_no\"]]\n",
    "    return \"This is an image with text written on it.\"\n",
    "\n",
    "tools = [image_ocr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:\n",
      "image_ocr\n",
      "=====\n",
      "Description:\n",
      "Perform Optical Character Recognition (OCR) on an image to extract texts.\n",
      "\n",
      "    Args:\n",
      "        image_file_path: the image file path\n",
      "=====\n",
      "Args:\n",
      "{'image_file_path': {'title': 'Image File Path', 'type': 'string'}}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Name:\\n{image_ocr.name}\")\n",
    "print(\"=====\")\n",
    "print(f\"Description:\\n{image_ocr.description}\")\n",
    "print(\"=====\")\n",
    "print(f\"Args:\\n{image_ocr.args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a image that contains texts.'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_ocr.invoke({\"image_file_path\": \"data/images/Step1校內徵選.jpg\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"\n",
    "When you receive a tool call response, use the output to format an answer to the orginal user question.\n",
    "\n",
    "You are a helpful assistant with tool calling capabilities.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "Here's a list of images that you can perform Optical Character Recognition (OCR) on to extract the texts in the image.\n",
    "The images are provided with their file path:\n",
    "1. \"data/images/Step1校內徵選.jpg\"\n",
    "\n",
    "Step1校內徵選的流程是什麼？\n",
    "\"\"\"\n",
    "\n",
    "# I would like to know what texts are written on \"Step1 校內徵選\" image. Please help me out.\n",
    "\n",
    "# I provided an image file with the file path: \"data/images/Step1校內徵選.jpg\".\n",
    "# Please perform Optical Character Recognition (OCR) on the image and show me the result.\n",
    "\n",
    "messages = [\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", user_prompt),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content: \n",
      "additional_kwargs: {'tool_calls': [{'id': 'call_fjk0', 'function': {'arguments': '{\"image_file_path\": \"data/images/Step1校內徵選.jpg\"}', 'name': 'image_ocr'}, 'type': 'function'}]}\n",
      "response_metadata: {'token_usage': {'completion_tokens': 27, 'prompt_tokens': 356, 'total_tokens': 383, 'completion_time': 0.098181818, 'prompt_time': 0.042432677, 'queue_time': 0.019514951999999995, 'total_time': 0.140614495}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_fcc3b74982', 'finish_reason': 'tool_calls', 'logprobs': None}\n",
      "type: ai\n",
      "name: None\n",
      "id: run-d10394e2-827a-477d-bd05-5b595067b156-0\n",
      "example: False\n",
      "tool_calls: [{'name': 'image_ocr', 'args': {'image_file_path': 'data/images/Step1校內徵選.jpg'}, 'id': 'call_fjk0', 'type': 'tool_call'}]\n",
      "invalid_tool_calls: []\n",
      "usage_metadata: {'input_tokens': 356, 'output_tokens': 27, 'total_tokens': 383}\n"
     ]
    }
   ],
   "source": [
    "# Invoke\n",
    "response = llm_with_tools.invoke(messages)\n",
    "\n",
    "for key, value in vars(response).items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async for chunk in llm_with_tools.astream(messages):\n",
    "#     print(chunk.tool_call_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
