{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent\n",
    "https://python.langchain.com/v0.1/docs/use_cases/tool_use/quickstart/#chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.tools import tool\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "\n",
    "from groq import Groq\n",
    "import base64\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 刪除環境變量\n",
    "if \"GROQ_API_KEY\" in os.environ:\n",
    "    del os.environ[\"GROQ_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/allen/Documents/code/Exchange_QA_Chatbot\n",
      "Successfully loaded env variables: True\n"
     ]
    }
   ],
   "source": [
    "# 找根目錄\n",
    "def find_project_root(current_path, marker=\".git\"):\n",
    "    current_path = Path(current_path).resolve()\n",
    "    for parent in current_path.parents:\n",
    "        if (parent / marker).exists():\n",
    "            return parent\n",
    "    return None\n",
    "\n",
    "current_path = os.getcwd()\n",
    "project_root = find_project_root(current_path, marker=\".git\")\n",
    "print(\"Project root:\", project_root)\n",
    "\n",
    "# Load .env file\n",
    "print(f\"Successfully loaded env variables: {load_dotenv(project_root / \".env\")}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded env variables:\n",
      "GROQ_API_KEY = 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\n"
     ]
    }
   ],
   "source": [
    "# Load env variables into python variables\n",
    "print(\"Loaded env variables:\")\n",
    "print(f\"GROQ_API_KEY = {os.getenv(\"GROQ_API_KEY\")}\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def image_interpreting(image_path: str) -> str:\n",
    "    \"\"\"Interpret, explain, analyse or describe the content and underlying logic of any given image and return in JSON format.\n",
    "\n",
    "    Args:\n",
    "        image_path: the image file path\n",
    "    \"\"\"\n",
    "    # Function to encode the image\n",
    "    def encode_image(image_path):\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "    base64_image = encode_image(image_path)\n",
    "    request_text = \"You are an assistant skilled at interpreting the content and underlying logic in images. Texts in this image are in Traditional Chinese. List what you observe in this image in JSON format.\"\n",
    "    client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        model=\"llama-3.2-90b-vision-preview\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {   \n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": request_text\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                        },\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "        top_p=1,\n",
    "        stream=False,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        stop=None,\n",
    "    )\n",
    "    return str(json.loads(chat_completion.choices[0].message.content))\n",
    "    # return \"{'title': 'Step2. 校外徵選', 'sections': [{'title': 'Nomination', 'description': '校方提名', 'dates': ['第一學期：2月至3月', '第二學期：8月至9月']}, {'title': 'Application', 'description': '學生申請文件', 'description2': '申請文件及期程依各交換校規定辦理'}, {'title': 'Result', 'description': '結果通知', 'description2': '母校通知', 'description3': '交換校通知'}, {'title': '學生負責事項', 'description': '包含住宿、簽證、交通、保險、健康檢查，限制提領帳戶...等(依各交換校及所在國家規定)'}], 'button': {'text': '看詳細資訊', 'color': 'orange'}}\"\n",
    "\n",
    "@tool\n",
    "def document_retrieval(string: str) -> str:\n",
    "    \"\"\"A powerful tool designed to retrieve relevant information from the robust document database using natural language questions or key phrases.\n",
    "\n",
    "    Args:\n",
    "        string: a sentence or a key phrase to retrieve relevant information from the database\n",
    "    \"\"\"\n",
    "    text = f\"\"\"\n",
    "1. 繳交文件\n",
    "2. 聯絡交換學校\n",
    "\"\"\"\n",
    "    return text\n",
    "\n",
    "tools = [image_interpreting, document_retrieval]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:\n",
      "image_interpreting\n",
      "=====\n",
      "Description:\n",
      "Interpret, explain or analyse the content and underlying logic of any given image and return in JSON format.\n",
      "\n",
      "    Args:\n",
      "        image_path: the image file path\n",
      "=====\n",
      "Args:\n",
      "{'image_path': {'title': 'Image Path', 'type': 'string'}}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Name:\\n{image_interpreting.name}\")\n",
    "print(\"=====\")\n",
    "print(f\"Description:\\n{image_interpreting.description}\")\n",
    "print(\"=====\")\n",
    "print(f\"Args:\\n{image_interpreting.args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'title': 'Step2. 校外徵選', 'sections': [{'title': 'Nomination', 'description': '校方提名', 'dates': ['第一學期：2月至3月', '第二學期：8月至9月']}, {'title': 'Application', 'description': '學生申請文件', 'description2': '申請文件及期程依各交換校規定辦理'}, {'title': 'Result', 'description': '結果通知', 'description2': '母校通知', 'description3': '交換校通知'}, {'title': '學生負責事項', 'description': '包含住宿、簽證、交通、保險、健康檢查，限制提領帳戶...等(依各交換校及所在國家規定)'}], 'button': {'text': '看詳細資訊', 'color': 'orange'}}\""
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_interpreting.invoke({\"image_path\": \"../data/images/Step2校外徵選.jpg\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    "    temperature=0.7,\n",
    "    max_retries=2,\n",
    "    api_key=GROQ_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allen/Documents/code/Exchange_QA_Chatbot/venv/lib/python3.12/site-packages/langsmith/client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a helpful assistant\n",
      "\n",
      "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{chat_history}\u001b[0m\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{input}\u001b[0m\n",
      "\n",
      "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{agent_scratchpad}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Get the prompt to use - can be replaced with any prompt that includes variables \"agent_scratchpad\" and \"input\"!\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the tool calling agent\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `image_interpreting` with `{'image_path': '../data/images/Step2校外徵選.jpg'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m{'title': 'Step2. 校外徵選', 'sections': [{'title': 'Nomination', 'description': '校方提名', 'dates': ['第一學期：2月至3月', '第二學期：8月至9月']}, {'title': 'Application', 'description': '學生申請文件', 'description2': '申請文件及期程依各交換校規定辦理'}, {'title': 'Result', 'description': '結果通知', 'description2': '母校通知', 'description3': '交換校通知'}, {'title': '學生負責事項', 'description': '包含住宿、簽證、交通、保險、健康檢查，限制提領帳戶...等(依各交換校及所在國家規定)'}], 'button': {'text': '看詳細資訊', 'color': 'orange'}}\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `document_retrieval` with `{'string': '學生負責事項'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m\n",
      "1. 繳交文件\n",
      "2. 聯絡交換學校\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m學生負責事項包含繳交文件、聯絡交換學校、住宿、簽證、交通、保險、健康檢查等，具體事項依各交換校及所在國家規定而定。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': '\\n\\'../data/images/Step2校外徵選.jpg\\'\\nRecommend using tool \"image_interpreting\" to interpret, explain or analyse the content and underlying logic of any given image and return in JSON format.\\nIf information in the image isn\\'t informative enough to answer the Human\\'s message below, you could use the powerful tool \"document_retrieval\" to retrieve relevant information from the robust document database using \\'Human\\'s message\\' below.\\n\\nHuman\\'s message:\\n\"學生負責事項有哪些？\"\\n',\n",
       " 'output': '學生負責事項包含繳交文件、聯絡交換學校、住宿、簽證、交通、保險、健康檢查等，具體事項依各交換校及所在國家規定而定。'}"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# user_prompt = \"'../data/images/Step2校外徵選.jpg' 請問在校方提名之後，學生要做什麼？\"\n",
    "# user_prompt = \"'../data/images/Step2校外徵選.jpg' 請問校方提名的第一學期是幾月至幾月？\"\n",
    "user_prompt = f\"\"\"\n",
    "'../data/images/Step2校外徵選.jpg'\n",
    "Recommend using tool \"image_interpreting\" to interpret, explain or analyse the content and underlying logic of any given image and return in JSON format.\n",
    "If information in the image isn't informative enough to answer the Human's message below, you could use the powerful tool \"document_retrieval\" to retrieve relevant information from the robust document database using 'Human's message' below.\n",
    "\n",
    "Human's message:\n",
    "\"學生負責事項有哪些？\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": user_prompt\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"\n",
    "When you receive a tool call response, use the output to format an answer to the orginal user question.\n",
    "\n",
    "You are a helpful assistant with tool calling capabilities.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "Here's a list of images that you can perform Optical Character Recognition (OCR) on to extract the texts in the image.\n",
    "The images are provided with their file path:\n",
    "1. \"data/images/Step1校內徵選.jpg\"\n",
    "\n",
    "Step1校內徵選的流程是什麼？\n",
    "\"\"\"\n",
    "\n",
    "# I would like to know what texts are written on \"Step1 校內徵選\" image. Please help me out.\n",
    "\n",
    "# I provided an image file with the file path: \"data/images/Step1校內徵選.jpg\".\n",
    "# Please perform Optical Character Recognition (OCR) on the image and show me the result.\n",
    "\n",
    "messages = [\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", user_prompt),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content: \n",
      "additional_kwargs: {'tool_calls': [{'id': 'call_fjk0', 'function': {'arguments': '{\"image_file_path\": \"data/images/Step1校內徵選.jpg\"}', 'name': 'image_ocr'}, 'type': 'function'}]}\n",
      "response_metadata: {'token_usage': {'completion_tokens': 27, 'prompt_tokens': 356, 'total_tokens': 383, 'completion_time': 0.098181818, 'prompt_time': 0.042432677, 'queue_time': 0.019514951999999995, 'total_time': 0.140614495}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_fcc3b74982', 'finish_reason': 'tool_calls', 'logprobs': None}\n",
      "type: ai\n",
      "name: None\n",
      "id: run-d10394e2-827a-477d-bd05-5b595067b156-0\n",
      "example: False\n",
      "tool_calls: [{'name': 'image_ocr', 'args': {'image_file_path': 'data/images/Step1校內徵選.jpg'}, 'id': 'call_fjk0', 'type': 'tool_call'}]\n",
      "invalid_tool_calls: []\n",
      "usage_metadata: {'input_tokens': 356, 'output_tokens': 27, 'total_tokens': 383}\n"
     ]
    }
   ],
   "source": [
    "# Invoke\n",
    "response = llm_with_tools.invoke(messages)\n",
    "\n",
    "for key, value in vars(response).items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
